# Decision-trees-and-ensemble

This repository contains Colab notebooks and corresponding video walkthroughs for implementing and understanding popular machine learning algorithms from scratch. Each algorithm is demonstrated with step-by-step code and explanations to help you grasp the core concepts and workflows. Additionally, ensemble techniques like Gradient Boosting, Random Forest, and AdaBoost are showcased along with their modern implementations using libraries like XGBoost, CatBoost, and LightGBM.

---

## **Table of Contents**
1. [Classic Gradient Boosting Method](#classic-gradient-boosting-method)  
   - [Video Explanation](#classic-gradient-boosting-method-video)
2. [Random Forest from Scratch](#random-forest-from-scratch)  
   - [Video Explanation](#random-forest-from-scratch-video)
3. [AdaBoost Algorithm from Scratch](#adaboost-algorithm-from-scratch)  
   - [Video Explanation](#adaboost-algorithm-from-scratch-video)
4. [Decision Trees from Scratch](#decision-trees-from-scratch)  
   - [Video Explanation](#decision-trees-from-scratch-video)
5. **Showcase of Gradient Boosting Techniques**  
   - [a) Classification Techniques](#showcase-classification-techniques)  
     - [Video Explanation](#showcase-classification-techniques-video)
   - [b) Regression Techniques](#showcase-regression-techniques)  
     - [Video Explanation](#showcase-regression-techniques-video)
   - [c) Ranking Techniques](#showcase-ranking-techniques)  
     - [Video Explanation](#showcase-ranking-techniques-video)

---

### **1. Classic Gradient Boosting Method**
This notebook demonstrates how to implement the Gradient Boosting Machine (GBM) algorithm from scratch in Python. You will learn the step-by-step process of building a boosting model and understand how weak learners (decision trees) are combined to form a strong learner.  
ðŸ‘‰ [Colab Notebook](#)  

#### **Classic Gradient Boosting Method - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

### **2. Random Forest from Scratch**
This notebook covers the implementation of the Random Forest algorithm from scratch. It demonstrates how multiple decision trees are aggregated to improve model performance and reduce overfitting.  
ðŸ‘‰ [Colab Notebook](#)  

#### **Random Forest from Scratch - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

### **3. AdaBoost Algorithm from Scratch**
Here, you will find an in-depth implementation of the AdaBoost algorithm, showcasing how weak classifiers are weighted and combined iteratively to build a robust ensemble model.  
ðŸ‘‰ [Colab Notebook](#)  

#### **AdaBoost Algorithm from Scratch - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

### **4. Decision Trees from Scratch**
This notebook details how to construct a Decision Tree from scratch, including splitting criteria, information gain, and tree-building algorithms like ID3, CART, and C4.5.  
ðŸ‘‰ [Colab Notebook](#)  

#### **Decision Trees from Scratch - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

### **5. Showcase of Gradient Boosting Techniques**
This section includes advanced implementations of Gradient Boosting techniques using popular libraries and frameworks.

#### **a) Classification Techniques**  
This notebook showcases classification techniques using XGBoost, CatBoost, LightGBM, Random Forest, AdaBoost, and Decision Tree classifiers. Practical examples and performance comparisons are provided.  
ðŸ‘‰ [Colab Notebook](#)  

#### **Classification Techniques - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

#### **b) Regression Techniques**  
This notebook demonstrates regression techniques using XGBoost, CatBoost, and LightGBM. It highlights how gradient boosting is adapted for regression tasks, with metrics for model evaluation.  
ðŸ‘‰ [Colab Notebook](#)  

#### **Regression Techniques - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

#### **c) Ranking Techniques**  
This notebook focuses on ranking techniques using XGBoost, CatBoost, and LightGBM. You will see practical applications of these algorithms in ranking-based problems, such as search result optimization.  
ðŸ‘‰ [Colab Notebook](#)  

#### **Ranking Techniques - Video Explanation**  
ðŸ‘‰ [YouTube Link](#)

---

## **Resources and References**
- [Gradient Boosting Documentation](https://docs.google.com/presentation/d/19j3wC-8_cz41CIm88F6kOFU8ys7zVcRfaBw6SImAeWc/edit#slide=id.ga2af525914_0_6955)  
- [Random Forest Example](https://github.com/veb-101/Machine-Learning-Algorithms/blob/master/Random%20Forest/random_forest.ipynb)  
- [AdaBoost Example](https://github.com/veb-101/Machine-Learning-Algorithms/tree/master/Boosting%20-%20AdaBoost)  
- [Decision Trees Example](https://github.com/veb-101/Machine-Learning-Algorithms/tree/master/Decision%20Trees)  
- [Hands-On Machine Learning Resources](https://github.com/ageron/handson-ml3)

